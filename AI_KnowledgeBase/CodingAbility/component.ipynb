{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d6e772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31076a0e",
   "metadata": {},
   "source": [
    "# LayerNorm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e977a",
   "metadata": {},
   "source": [
    "\n",
    "## 核心思想\n",
    "\n",
    "对每个样本（token）自己的所有特征独立做“均值为 0、标准差为 1”的归一化，完全不依赖 batch 大小。\n",
    "\n",
    "![LN](pic/LN.png)\n",
    "\n",
    "## 为什么这么做\n",
    "\n",
    "由于对每个样本做了归一化，首先带来的好处是梯度是稳定的，不会过大或者过小，而梯度稳定则是训练稳定的重要前提。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0deae0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class layernorm(nn.Module):\n",
    "    def __init__(self, input_dim, eps=1e-5) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.input_dim = input_dim\n",
    "        self.weight = nn.Parameter(torch.ones(self.input_dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.input_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim= -1, keepdim=True)\n",
    "        var = x.var(dim= -1, keepdim=True, unbiased=False)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.weight * x_norm + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee1a77c",
   "metadata": {},
   "source": [
    "## 代码测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b295ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "        [-1.3416, -0.4472,  0.4472,  1.3416]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1., 2., 3., 4.], [10., 20., 30., 40.]], dtype=torch.float32)\n",
    "ln = layernorm(4)\n",
    "x_norm = ln(x)\n",
    "x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000be05",
   "metadata": {},
   "source": [
    "## 一些值得注意的\n",
    "\n",
    "### `nn.Parameter()`的用法\n",
    "\n",
    "* `nn.Parameter()`用于自定义可学习的参数，是tensor的子类。\n",
    "\n",
    "    当神经网络继承自nn.Module时，如果其中存在一些参数是需要纳入模型本身的参数里参与反向传播过程的，则可以使用该方法创建参数。\n",
    "\n",
    "* 创建方法即为使用`nn.Parameter()`方法包裹tensor:\n",
    "\n",
    "    `self.weight = nn.Parameter(torch.ones(self.input_dim))`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e4fbd",
   "metadata": {},
   "source": [
    "# RMS Norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b2fa3f",
   "metadata": {},
   "source": [
    "\n",
    "## 核心思想\n",
    "\n",
    "![LN](pic/RMS_Norm.png)\n",
    "\n",
    "RMSNorm 主要是在 LayerNorm 的基础上去掉了减均值这一项，其计算效率更高且没有降低性能。\n",
    "\n",
    "因为原论文中指出LayerNorm发挥作用主要是其缩放不变性（即除以标准差），而不是平移不变性（即减去均值）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "612107dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMS_Norm(nn.Module):\n",
    "    def __init__(self, input_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.input_dim = input_dim\n",
    "        self.weight = nn.Parameter(torch.ones(self.input_dim))\n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        return x * self.weight / rms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaca1e2",
   "metadata": {},
   "source": [
    "## 代码测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef85041a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "        [-1.3416, -0.4472,  0.4472,  1.3416]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1., 2., 3., 4.], [10., 20., 30., 40.]], dtype=torch.float32)\n",
    "rmsn = RMS_Norm(4)\n",
    "x_norm = ln(x)\n",
    "x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c6663f",
   "metadata": {},
   "source": [
    "# Batch Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afebbf1",
   "metadata": {},
   "source": [
    "# MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c8df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.w1 = nn.Linear(input_dim, input_dim * 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.w2 = nn.Linear(input_dim * 4, input_dim)\n",
    "    def forward(self, x):\n",
    "        return self.w2(self.relu(self.w1(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363d96d",
   "metadata": {},
   "source": [
    "## 代码测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a88c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3, 24)\n",
    "mlp = MLP(24)\n",
    "x = mlp(x)\n",
    "print(\"x.shape:\", x.shape)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0837025f",
   "metadata": {},
   "source": [
    "# SwiGLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40ec6f8",
   "metadata": {},
   "source": [
    "## 常见的激活函数\n",
    "* ReLU\n",
    "\n",
    "    $ReLU(x) = \\max(0, x)$\n",
    "\n",
    "* GeLU\n",
    "\n",
    "    论文《Gaussian Error Linear Units（GELUs）》提出了GELU，GeLU激活函数是处处可微的非线性函数，比ReLU更平滑\n",
    "\n",
    "* Swish\n",
    "\n",
    "    论文《Swish: a Self-Gated Activation Function》提出了Swish，这也是对带有非零负值梯度的ReLU平滑版本，同样是个处处可微的非线性函数，且有一个参数beta用于控制函数的形状。\n",
    "\n",
    "* Silu\n",
    "\n",
    "    beta=1的swish函数\n",
    "\n",
    "![comparation](pic/af.png)\n",
    "\n",
    "## GLU\n",
    "\n",
    "GLU（Gated Linear Units）其实不算是一种激活函数，而是一种神经网络层。它是一个线性变换后面接门控机制的结构。其中门控机制是一个sigmoid函数用来控制信息能够通过多少。\n",
    "\n",
    "$GLU(x,W,V,b,c) = \\sigma (xW + b) \\odot (xV + c)$\n",
    "\n",
    "这里$\\sigma$代表SIGMOD函数，$\\odot$代表逐元素乘。\n",
    "\n",
    "## SwiGLU\n",
    "\n",
    "把上面公式的激活函数换成swish函数：\n",
    "\n",
    "$GLU(x,W,V,b,c) = Swish_1 (xW + b) \\odot (xV + c)$\n",
    "\n",
    "就是SwiGLU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18845e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, input_dim,  multiple_of=64) -> None:\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.hidden_dim = 4 * self.input_dim\n",
    "        self.hidden_dim = int(2 * self.hidden_dim / 3)\n",
    "        self.hidden_dim = multiple_of * ((self.hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.w2 = nn.Linear(self.hidden_dim, self.input_dim)\n",
    "        self.w3 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.silu = nn.SiLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w2(self.silu(self.w1(x)) * self.w3(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98134fc4",
   "metadata": {},
   "source": [
    "## 代码测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb5f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3, 24)\n",
    "mlp = SwiGLU(24)\n",
    "x = mlp(x)\n",
    "print(\"x.shape:\", x.shape)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc2e25",
   "metadata": {},
   "source": [
    "## 值得一提的是\n",
    "\n",
    "### 为什么要重新计算隐藏层维度\n",
    "\n",
    "传统FFN使用4倍的input_dim作为hidden_dim（见上述MLP代码），此时不妨do the math：\n",
    "\n",
    "* 传统FFN使用了两个线性层，参数量为 $ dim \\times 4dim \\times 2$\n",
    "* SwiGLU使用了三个线性层，参数量为 $ dim \\times hidden \\_ dim  \\times 3$\n",
    "* 想要保证传统FFN与SwiGLU参数量上不发生太大变化，那么 $ hidden \\_ dim $选择为 $ \\cfrac{2}{3} \\times 4dim $ 是最好的\n",
    "* 然而NVIDIA GPU 的 Tensor Core 要求矩阵维度是 8/16/32/256 的倍数才能发挥最大算力，因此还得将$ hidden \\_ dim $取值为8/16/32/256 的倍数，同时接近$ \\cfrac{2}{3} \\times 4dim $\n",
    "\n",
    "### 取整技巧\n",
    "\n",
    "其中\n",
    "\n",
    "        self.hidden_dim = multiple_of * ((self.hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "这个操作是将self.hidden_dim重新取值为可以被multiple_of整除的数，数学表达为：\n",
    "\n",
    "$ceil(x / m) = (x + m - 1) // m$\n",
    "\n",
    "* **情况1**: x 恰好是 m 的倍数，设 x = k*m\n",
    "\n",
    "    (k*m + m - 1) // m = k + (m-1)//m = k + 0 = k  ✓\n",
    "\n",
    "* **情况2**: x 有余数，设 x = k*m + r (0 < r < m)\n",
    "\n",
    "    (k*m + r + m - 1) // m = k + (r + m - 1)//m = k + 1  ✓\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6637375",
   "metadata": {},
   "source": [
    "# DecoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf5f8b1",
   "metadata": {},
   "source": [
    "# EncoderLayer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLaMa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
