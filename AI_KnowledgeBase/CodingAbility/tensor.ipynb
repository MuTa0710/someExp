{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c09222",
   "metadata": {},
   "source": [
    "# PyTorch Tensor 常用计算方法详解\n",
    "\n",
    "PyTorch 的 `Tensor` 类提供了极其丰富的方法，涵盖了从基础数学运算到高级线性代数的所有需求。对于开发者来说，掌握这些方法可以写出更简洁、高效的代码。\n",
    "\n",
    "PyTorch 的运算通常有两种调用方式，**功能基本等价**：\n",
    "1.  **函数式**：`torch.mean(x)`\n",
    "2.  **方法式**：`x.mean()`\n",
    "\n",
    "以下是按功能分类的常用 Tensor 计算方法详解，包含代码示例和注意事项。\n",
    "\n",
    "---\n",
    "\n",
    "## 一、归约运算（Reduction Operations）\n",
    "**作用**：将张量的多个元素聚合为一个值（或沿某个维度聚合）。\n",
    "**关键点**：注意 `dim` 参数，决定沿哪个维度计算。\n",
    "\n",
    "| 方法 | 说明 | 示例 |\n",
    "| :--- | :--- | :--- |\n",
    "| `mean(dim)` | 计算均值 | `x.mean(dim=0)` |\n",
    "| `sum(dim)` | 计算求和 | `x.sum(dim=1)` |\n",
    "| `prod(dim)` | 计算乘积 | `x.prod()` |\n",
    "| `std(dim, unbiased)` | 计算标准差 | `x.std(dim=0, unbiased=False)` |\n",
    "| `var(dim, unbiased)` | 计算方差 | `x.var(dim=0)` |\n",
    "| `max(dim)` | 最大值 (返回值，索引) | `val, idx = x.max(dim=1)` |\n",
    "| `min(dim)` | 最小值 (返回值，索引) | `val, idx = x.min(dim=1)` |\n",
    "| `argmax(dim)` | 最大值的索引 | `x.argmax(dim=1)` |\n",
    "| `argmin(dim)` | 最小值的索引 | `x.argmin()` |\n",
    "| `norm(p, dim)` | 计算范数 (L1, L2 等) | `x.norm(p=2, dim=1)` |\n",
    "| `cumsum(dim)` | 累积求和 | `x.cumsum(dim=0)` |\n",
    "\n",
    "**代码示例**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e853a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "x = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\n",
    "\n",
    "print(x.mean())          # 标量：3.5\n",
    "print(x.mean(dim=0))     # 按列均值：[2.5, 3.5, 4.5]\n",
    "print(x.sum(dim=1))      # 按行求和：[6., 15.]\n",
    "print(x.std(dim=1))      # 按行标准差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0cfc7",
   "metadata": {},
   "source": [
    "## 二、逐元素数学运算（Element-wise Math）\n",
    "**作用**：对张量中的每个元素独立进行数学计算。\n",
    "**关键点**：支持广播机制（Broadcasting）。\n",
    "\n",
    "| 方法 | 说明 | 示例 |\n",
    "| :--- | :--- | :--- |\n",
    "| `add(other)` | 加法 (`+`) | `x.add(1)` 或 `x + 1` |\n",
    "| `sub(other)` | 减法 (`-`) | `x.sub(1)` 或 `x - 1` |\n",
    "| `mul(other)` | 乘法 (`*`) | `x.mul(2)` 或 `x * 2` |\n",
    "| `div(other)` | 除法 (`/`) | `x.div(2)` 或 `x / 2` |\n",
    "| `pow(exp)` | 幂运算 | `x.pow(2)` 或 `x ** 2` |\n",
    "| `sqrt()` | 平方根 | `x.sqrt()` |\n",
    "| `rsqrt()` | 倒数平方根 (`1/sqrt`) | `x.rsqrt()` |\n",
    "| `square()` | 平方 | `x.square()` |\n",
    "| `abs()` | 绝对值 | `x.abs()` |\n",
    "| `neg()` | 取负 | `x.neg()` 或 `-x` |\n",
    "| `exp()` | 指数 (`e^x`) | `x.exp()` |\n",
    "| `log()` | 自然对数 (`ln`) | `x.log()` |\n",
    "| `log10()` / `log2()` | 对数 | `x.log10()` |\n",
    "| `sin()`, `cos()`, `tan()` | 三角函数 | `x.sin()` |\n",
    "| `floor()`, `ceil()`, `round()` | 下取整，上取整，四舍五入 | `x.floor()` |\n",
    "| `clamp(min, max)` | 截断 (类似 ReLU) | `x.clamp(min=0)` |\n",
    "\n",
    "\n",
    "**代码示例**：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673fc51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 4., 9.])\n",
    "\n",
    "print(x.sqrt())       # [1., 2., 3.]\n",
    "print(x.pow(0.5))     # [1., 2., 3.] (等价 sqrt)\n",
    "print(x.clamp(min=0, max=5)) # 限制范围\n",
    "print(x.exp())        # [e^1, e^4, e^9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b8fdd0",
   "metadata": {},
   "source": [
    "## 三、比较与逻辑运算（Comparison & Logic）\n",
    "**作用**：生成布尔张量（Boolean Tensor），常用于 Mask 操作。\n",
    "\n",
    "| 方法 | 说明 | 示例 |\n",
    "| :--- | :--- | :--- |\n",
    "| `eq(other)` | 等于 (`==`) | `x.eq(0)` |\n",
    "| `ne(other)` | 不等于 (`!=`) | `x.ne(0)` |\n",
    "| `gt(other)` | 大于 (`>`) | `x.gt(0)` |\n",
    "| `lt(other)` | 小于 (`<`) | `x.lt(0)` |\n",
    "| `ge(other)` | 大于等于 (`>=`) | `x.ge(0)` |\n",
    "| `le(other)` | 小于等于 (`<=`) | `x.le(0)` |\n",
    "| `bool()` | 转换为布尔类型 | `x.bool()` |\n",
    "| `masked_fill(mask, val)`| 按掩码填充值 | `x.masked_fill(x<0, 0)` |\n",
    "\n",
    "**代码示例**：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d68be",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([-1., 0., 1., 2.])\n",
    "\n",
    "mask = x.gt(0)          # [False, False, True, True]\n",
    "print(x[mask])          # [1., 2.] (布尔索引)\n",
    "print(x.masked_fill(x < 0, 0)) # [-0., 0., 1., 2.] (负数变 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1237c808",
   "metadata": {},
   "source": [
    "\n",
    "## 四、线性代数运算（Linear Algebra）\n",
    "**作用**：矩阵乘法、转置等。\n",
    "\n",
    "| 方法 | 说明 | 示例 |\n",
    "| :--- | :--- | :--- |\n",
    "| `matmul(other)` | 矩阵乘法 (`@`) | `x.matmul(y)` 或 `x @ y` |\n",
    "| `mm(other)` | 2D 矩阵乘法 | `x.mm(y)` (严格 2D) |\n",
    "| `bmm(other)` | 批量矩阵乘法 | `x.bmm(y)` (3D: [B, N, M]) |\n",
    "| `t()` | 2D 转置 | `x.t()` |\n",
    "| `transpose(dim0, dim1)` | 任意维度交换 | `x.transpose(0, 1)` |\n",
    "| `permute(*dims)` | 任意维度重排 | `x.permute(2, 0, 1)` |\n",
    "| `inverse()` | 矩阵求逆 | `x.inverse()` |\n",
    "| `det()` | 行列式 | `x.det()` |\n",
    "\n",
    "**代码示例**：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c02990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4, 5)\n",
    "\n",
    "C = A.matmul(B)     # 形状 [3, 5]\n",
    "D = A @ B           # 等价写法\n",
    "print(A.t().shape)  # [4, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17d1ecd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 五、形状与内存操作（Shape & Memory）\n",
    "虽然不算“计算”，但通常与计算紧密配合。\n",
    "\n",
    "| 方法 | 说明 | 示例 |\n",
    "| :--- | :--- | :--- |\n",
    "| `view(*shape)` | 改变形状 (需内存连续) | `x.view(-1, 10)` |\n",
    "| `reshape(*shape)` | 改变形状 (更智能) | `x.reshape(3, -1)` |\n",
    "| `squeeze(dim)` | 去除维度为 1 的轴 | `x.squeeze()` |\n",
    "| `unsqueeze(dim)` | 增加维度为 1 的轴 | `x.unsqueeze(0)` |\n",
    "| `flatten(start, end)`| 展平 | `x.flatten(1, 2)` |\n",
    "| `contiguous()` | 使内存连续 | `x.transpose(0,1).contiguous()` |\n",
    "| `item()` | 获取标量值 (Python 数) | `x.sum().item()` |\n",
    "| `numpy()` | 转为 NumPy 数组 | `x.numpy()` |\n",
    "\n",
    "---\n",
    "\n",
    "## 六、梯度与设备管理（Gradient & Device）\n",
    "深度学习特有的操作。\n",
    "\n",
    "| 方法 | 说明 | 示例 |\n",
    "| :--- | :--- | :--- |\n",
    "| `backward()` | 反向传播 | `loss.backward()` |\n",
    "| `detach()` | 脱离计算图 (无梯度) | `x.detach()` |\n",
    "| `requires_grad_(flag)`| 设置是否需要梯度 | `x.requires_grad_(True)` |\n",
    "| `to(device)` | 移动设备 (CPU/CUDA) | `x.to('cuda')` |\n",
    "| `cuda()` / `cpu()` | 快捷移动设备 | `x.cuda()` |\n",
    "| `type(dtype)` | 转换数据类型 | `x.float()` / `x.half()` |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLaMa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
